## –ê–Ω–∞–ª–∏–∑: –û–±—É—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ vs Ready-to-use AI

**Date:** 2025-10-11  
**Status:** üìä Analysis Only (No code changes)

---

## üéØ –í–∞—à –≤–æ–ø—Ä–æ—Å

> "–ù–∞–¥–æ —Å–¥–µ–ª–∞—Ç—å –Ω–∞—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ò–ò, –æ–±—É—á–∏—Ç—å –Ω–∞ –º–∞—Å—Å–∏–≤–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π.  
> –û—Ü–µ–Ω–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ NotebookLM –æ—Ç Google –∏ –¥—Ä—É–≥–∏—Ö —Ä–µ—à–µ–Ω–∏–π."

---

## üìä –ê–Ω–∞–ª–∏–∑ Google NotebookLM

### –ß—Ç–æ —Ç–∞–∫–æ–µ NotebookLM?

**NotebookLM** - —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ—Ç Google –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –∑–∞–º–µ—Ç–∫–∞–º–∏, –ù–û:

‚ùå **–ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏, –ø–æ—Ç–æ–º—É —á—Ç–æ:**

1. **–ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ vision –º–æ–¥–µ–ª–µ–π**
   - NotebookLM —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
   - –ù–µ –∏–º–µ–µ—Ç —Ñ—É–Ω–∫—Ü–∏–π computer vision
   - –ù–µ –º–æ–∂–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è—Ö

2. **–ù–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è image analysis**
   - –§–æ–∫—É—Å –Ω–∞ RAG (Retrieval-Augmented Generation)
   - –†–∞–±–æ—Ç–∞–µ—Ç —Å PDF, —Ç–µ–∫—Å—Ç–æ–º, –∑–∞–º–µ—Ç–∫–∞–º–∏
   - –ù–µ—Ç API –¥–ª—è mass processing

3. **–ù–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –¥–ª—è production**
   - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
   - –ù–µ—Ç batch processing
   - –ù–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ API

---

## ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏

### –í–∞—Ä–∏–∞–Ω—Ç 1: Fine-tune —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π vision –º–æ–¥–µ–ª–∏ (–û–ü–¢–ò–ú–ê–õ–¨–ù–û)

#### OpenAI GPT-4 Vision Fine-tuning

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è **–ù–ï –¥–æ—Å—Ç—É–ø–Ω–æ** (–ø–æ–∫–∞)
- OpenAI –Ω–µ –æ—Ç–∫—Ä—ã–ª–∞ fine-tuning –¥–ª—è GPT-4 Vision
- –î–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è GPT-3.5 Turbo, GPT-4 (text only)
- –û–∂–∏–¥–∞–µ—Ç—Å—è –≤ 2025-2026

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPT-4 Vision as-is —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏

---

#### Google Vertex AI (Vision AI) ‚úÖ –î–û–°–¢–£–ü–ù–û

**–ß—Ç–æ —ç—Ç–æ:**
- –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ Google Cloud –¥–ª—è AI/ML
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç fine-tuning vision –º–æ–¥–µ–ª–µ–π
- AutoML Vision –¥–ª—è custom models

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
```python
# AutoML Vision
# 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å dataset (—Ñ–æ—Ç–æ + labels)
# 2. –û–±—É—á–∏—Ç—å custom –º–æ–¥–µ–ª—å
# 3. –î–µ–ø–ª–æ–∏—Ç—å –∫–∞–∫ API endpoint
# 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è mass processing
```

**–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- –û–±—É—á–µ–Ω–∏–µ: $3.15/—á–∞—Å (node hour)
- Training time: 8-24 —á–∞—Å–∞
- **Training cost: $25-75 –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ**
- Inference: $1.50/1000 predictions
- **Inference for 50k: $75**

**–ò—Ç–æ–≥–æ:** $100-150 (–æ–±—É—á–µ–Ω–∏–µ + inference)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö
- ‚úÖ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π inference
- ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –¢—Ä–µ–±—É–µ—Ç labeled dataset (min 1000 —Ñ–æ—Ç–æ)
- ‚ùå –ù—É–∂–Ω–∞ —Ä–∞–∑–º–µ—Ç–∫–∞ (—Ä—É—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞)
- ‚ùå –°–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

---

#### Azure Custom Vision ‚úÖ –î–û–°–¢–£–ü–ù–û

**–ß—Ç–æ —ç—Ç–æ:**
- Microsoft Azure service –¥–ª—è custom vision models
- –ü—Ä–æ—Å—Ç–æ–π UI –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- Minimal code required

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
```
1. Upload photos with labels (1-5 condition)
2. Train model (automatic)
3. Deploy as endpoint
4. Call API for predictions
```

**–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- Training: –ë–ï–°–ü–õ–ê–¢–ù–û (–ø–µ—Ä–≤—ã–µ 2 –ø—Ä–æ–µ–∫—Ç–∞)
- Predictions: $1/1000 images
- **For 50k: $50**

**–ò—Ç–æ–≥–æ:** $50 (—Å–∞–º—ã–π –¥–µ—à–µ–≤—ã–π!)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –°–∞–º—ã–π –¥–µ—à–µ–≤—ã–π
- ‚úÖ –ü—Ä–æ—Å—Ç–æ–π UI
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –¢—Ä–µ–±—É–µ—Ç labeled dataset
- ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: Open-source –º–æ–¥–µ–ª–∏ (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô –ö–û–ù–¢–†–û–õ–¨)

#### A. CLIP + Fine-tuning ‚úÖ –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø

**–ß—Ç–æ —ç—Ç–æ:**
- OpenAI's CLIP (open-source)
- Pre-trained –Ω–∞ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –ú–æ–∂–Ω–æ fine-tune –Ω–∞ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
```python
import torch
from transformers import CLIPProcessor, CLIPModel

# 1. Load pre-trained CLIP
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 2. Fine-tune on apartment photos
# - Dataset: 5,000-10,000 labeled photos
# - Labels: 1-5 condition rating
# - Training: 2-4 hours on GPU

# 3. Inference
inputs = processor(images=image, return_tensors="pt")
outputs = model(**inputs)
condition_score = predict_condition(outputs)
```

**–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- –û–±—É—á–µ–Ω–∏–µ: **–ë–ï–°–ü–õ–ê–¢–ù–û** (—Å–≤–æ–π GPU) –∏–ª–∏ $50-100 (cloud GPU)
- Inference: **–ë–ï–°–ü–õ–ê–¢–ù–û** (self-hosted)
- **Total: $0-100**

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π inference
- ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- ‚úÖ –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ
- ‚úÖ –ù–µ—Ç rate limits

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –¢—Ä–µ–±—É–µ—Ç ML —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É
- ‚ùå –ù—É–∂–µ–Ω labeled dataset
- ‚ùå –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (GPU server)

---

#### B. ResNet50 + Transfer Learning

**–ü–æ–¥—Ö–æ–¥:**
```python
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# 1. Load pre-trained ResNet50 (ImageNet)
base_model = ResNet50(weights='imagenet', include_top=False)

# 2. Add custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
predictions = Dense(5, activation='softmax')(x)  # 5 classes

model = Model(inputs=base_model.input, outputs=predictions)

# 3. Fine-tune on apartment photos
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.fit(train_dataset, epochs=10)
```

**–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- Training: $20-50 (Google Colab Pro)
- Inference: **–ë–ï–°–ü–õ–ê–¢–ù–û** (self-hosted)
- **Total: $20-50**

---

### –í–∞—Ä–∏–∞–Ω—Ç 3: Hybrid Approach (–ü–†–ê–ö–¢–ò–ß–ù–´–ô)

#### –ö–æ–º–±–∏–Ω–∞—Ü–∏—è: Zero-shot + Few-shot learning

**–ü–æ–¥—Ö–æ–¥:**
```python
# 1. –°–æ–±—Ä–∞—Ç—å 100-200 —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
examples = {
    "excellent": ["photo1.jpg", "photo2.jpg", ...],  # 20 –ø—Ä–∏–º–µ—Ä–æ–≤
    "good": ["photo21.jpg", ...],  # 20 –ø—Ä–∏–º–µ—Ä–æ–≤
    "fair": ["photo41.jpg", ...],  # 20 –ø—Ä–∏–º–µ—Ä–æ–≤
    "poor": ["photo61.jpg", ...],  # 20 –ø—Ä–∏–º–µ—Ä–æ–≤
    "very_poor": ["photo81.jpg", ...],  # 20 –ø—Ä–∏–º–µ—Ä–æ–≤
}

# 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø—Ä–æ–º–ø—Ç–µ GPT-4 Vision
prompt = """
–í–æ—Ç –ø—Ä–∏–º–µ—Ä—ã –∫–≤–∞—Ä—Ç–∏—Ä —Ä–∞–∑–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è:

Excellent (5/5):
[–ø–æ–∫–∞–∑–∞—Ç—å 2-3 —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ]

Good (4/5):
[–ø–æ–∫–∞–∑–∞—Ç—å 2-3 —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ]

...

–û—Ü–µ–Ω–∏ —ç—Ç—É –∫–≤–∞—Ä—Ç–∏—Ä—É –ø–æ —Ç–æ–π –∂–µ —à–∫–∞–ª–µ:
[–Ω–æ–≤–æ–µ —Ñ–æ—Ç–æ]
"""

# 3. Few-shot learning —Ä–∞–±–æ—Ç–∞–µ—Ç!
```

**–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: –æ–¥–∏–Ω —Ä–∞–∑ –≤ –ø—Ä–æ–º–ø—Ç
- Cost per image: $0.00425 (low detail)
- **For 50k: $213** (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ë–µ–∑ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å vs zero-shot
- ‚úÖ –õ–µ–≥–∫–æ –æ–±–Ω–æ–≤–ª—è—Ç—å —ç—Ç–∞–ª–æ–Ω—ã

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –î–æ—Ä–æ–∂–µ —á–µ–º fine-tuned –º–æ–¥–µ–ª—å
- ‚ùå –ú–µ–Ω—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤

| –†–µ—à–µ–Ω–∏–µ | Training Cost | Inference Cost (50k) | Total | Complexity | Accuracy |
|---------|---------------|---------------------|-------|------------|----------|
| **GPT-4 Vision (as-is)** | $0 | $213 | $213 | –ù–∏–∑–∫–∞—è | –•–æ—Ä–æ—à–∞—è |
| **GPT-4 + Few-shot** | $0 | $213 | $213 | –ù–∏–∑–∫–∞—è | –û—Ç–ª–∏—á–Ω–∞—è |
| **Azure Custom Vision** | $0 | $50 | **$50** | –°—Ä–µ–¥–Ω—è—è | –û—Ç–ª–∏—á–Ω–∞—è |
| **Google Vertex AI** | $50 | $75 | $125 | –°—Ä–µ–¥–Ω—è—è | –û—Ç–ª–∏—á–Ω–∞—è |
| **CLIP Fine-tune** | $50 | $0 | **$50** | –í—ã—Å–æ–∫–∞—è | –û—Ç–ª–∏—á–Ω–∞—è |
| **ResNet50 Transfer** | $30 | $0 | **$30** | –í—ã—Å–æ–∫–∞—è | –•–æ—Ä–æ—à–∞—è |

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏

### –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ (1-2 –º–µ—Å—è—Ü–∞):

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** GPT-4 Vision —Å –≤—ã–±–æ—Ä–æ—á–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º (30%)

```
–°—Ç–æ–∏–º–æ—Å—Ç—å: $64 (–≤–∞–∂–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è)
–í—Ä–µ–º—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è: 1 –¥–µ–Ω—å
–ö–∞—á–µ—Å—Ç–≤–æ: –•–æ—Ä–æ—à–µ–µ (85-90% accuracy)
```

**–ü–æ—á–µ–º—É:**
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (—É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ!)
- ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã
- ‚úÖ –ù–µ—Ç –Ω—É–∂–¥—ã –≤ labeled dataset
- ‚úÖ –ú–æ–∂–Ω–æ –Ω–∞—á–∞—Ç—å —Å—Ä–∞–∑—É

---

### –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ (3-6 –º–µ—Å—è—Ü–µ–≤):

**–û–±—É—á–∏—Ç—å:** Azure Custom Vision –∏–ª–∏ CLIP fine-tuning

**–ü–ª–∞–Ω:**

#### –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ dataset (2-4 –Ω–µ–¥–µ–ª–∏)

```
1. –°–æ–±—Ä–∞—Ç—å 5,000-10,000 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∫–≤–∞—Ä—Ç–∏—Ä
2. –†–∞–∑–º–µ—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é:
   - 1000 —Ñ–æ—Ç–æ √ó 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π = 5,000 labeled
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å crowd-sourcing (–Ø–Ω–¥–µ–∫—Å.–¢–æ–ª–æ–∫–∞)
   - –°—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ç–∫–∏: $500-1000
3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏
```

#### –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (1 –Ω–µ–¥–µ–ª—è)

**–í–∞—Ä–∏–∞–Ω—Ç A: Azure Custom Vision**
```bash
# 1. Upload labeled dataset
az cognitiveservices customvision upload-images

# 2. Train model (automatic)
az cognitiveservices customvision train

# 3. Deploy endpoint
az cognitiveservices customvision publish

# –°—Ç–æ–∏–º–æ—Å—Ç—å: $0 (–±–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
```

**–í–∞—Ä–∏–∞–Ω—Ç B: CLIP Fine-tune (self-hosted)**
```python
# 1. Prepare dataset
train_dataset = ApartmentDataset(
    photos_dir="data/apartments/",
    labels="data/labels.csv"
)

# 2. Fine-tune CLIP
from clip_finetune import train_model
model = train_model(
    base_model="openai/clip-vit-base-patch32",
    dataset=train_dataset,
    epochs=10,
    batch_size=32
)

# 3. Save model
model.save("models/apartment_condition_v1.pt")

# –°—Ç–æ–∏–º–æ—Å—Ç—å: $50-100 (Google Colab Pro GPU)
```

#### –≠—Ç–∞–ø 3: Production deployment (1 –Ω–µ–¥–µ–ª—è)

```python
# Deploy as API
from fastapi import FastAPI
from apartment_model import predict_condition

app = FastAPI()

@app.post("/analyze")
async def analyze_photo(photo_url: str):
    score = predict_condition(photo_url)
    return {"condition_score": score}

# Cost: $0 per prediction (self-hosted)
```

#### –≠—Ç–∞–ø 4: Mass processing

```bash
# Analyze all 50,000 listings
python -m etl.ai_evaluator.cli analyze-batch \
    --model custom \
    --endpoint http://localhost:8000/analyze

# Cost: $0 (self-hosted)
# Time: ~6 hours (50k photos)
```

**–ò—Ç–æ–≥–æ:**
- Dataset creation: $500-1000 (—Ä–∞–∑–º–µ—Ç–∫–∞)
- Training: $0-100 (Azure/Colab)
- Inference: **$0** (self-hosted)
- **Total: $600-1100 –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ**
- **Then: $0 per 50k** üéâ

---

### –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ (6-12 –º–µ—Å—è—Ü–µ–≤):

**–°–æ–∑–¥–∞—Ç—å:** –°–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ $0 –Ω–∞ inference (–±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
- ‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (trained –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
- ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
- ‚úÖ –ú–æ–∂–Ω–æ –ø—Ä–æ–¥–∞–≤–∞—Ç—å –∫–∞–∫ —Å–µ—Ä–≤–∏—Å

---

## üî¨ –ü–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Fine-tuning

### –®–∞–≥ 1: –°–±–æ—Ä training dataset (–ö–†–ò–¢–ò–ß–ù–û)

#### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:

```
–í—Å–µ–≥–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: 5,000-10,000
–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:
  - Excellent (5): 1,000 —Ñ–æ—Ç–æ
  - Good (4): 2,000 —Ñ–æ—Ç–æ (–±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤)
  - Fair (3): 3,000 —Ñ–æ—Ç–æ (—Ç–∏–ø–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏)
  - Poor (2): 2,000 —Ñ–æ—Ç–æ
  - Very Poor (1): 1,000 —Ñ–æ—Ç–æ

–ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ç–∫–∏: –ú–∏–Ω–∏–º—É–º 2 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ü–µ–Ω–∫–∏
```

#### –û—Ç–∫—É–¥–∞ –≤–∑—è—Ç—å:

**–ò—Å—Ç–æ—á–Ω–∏–∫ 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPT-4 Vision –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏**
```python
# 1. –°–æ–±—Ä–∞—Ç—å 10,000 —Ñ–æ—Ç–æ –∏–∑ CIAN
# 2. –†–∞–∑–º–µ—Ç–∏—Ç—å —á–µ—Ä–µ–∑ GPT-4 Vision ($0.00425 √ó 10,000 = $43)
# 3. –†—É—á–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è 20% ($200 –Ω–∞ crowd-sourcing)
# 4. –ò—Ç–æ–≥–æ: $243 –∑–∞ labeled dataset
```

**–ò—Å—Ç–æ—á–Ω–∏–∫ 2: Crowd-sourcing (–Ø–Ω–¥–µ–∫—Å.–¢–æ–ª–æ–∫–∞)**
```python
# –ó–∞–¥–∞–Ω–∏–µ: "–û—Ü–µ–Ω–∏—Ç–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã –ø–æ —Ñ–æ—Ç–æ (1-5)"
# –°—Ç–æ–∏–º–æ—Å—Ç—å: $0.05-0.10 –∑–∞ —Ñ–æ—Ç–æ
# 10,000 —Ñ–æ—Ç–æ √ó $0.07 = $700
# –ö–∞—á–µ—Å—Ç–≤–æ: –í—ã—Å–æ–∫–æ–µ (3 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ü–µ–Ω–∫–∏)
```

**–ò—Å—Ç–æ—á–Ω–∏–∫ 3: Hybrid (GPT-4 + manual verification)**
```python
# 1. GPT-4 Vision: 10,000 —Ñ–æ—Ç–æ ($43)
# 2. –†—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–æ—Ä–Ω—ã—Ö (30%): $200
# –ò—Ç–æ–≥–æ: $243
# –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ
```

---

### –®–∞–≥ 2: –í—ã–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏

#### –í–∞—Ä–∏–∞–Ω—Ç A: CLIP Fine-tuning (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)

**–ü–æ—á–µ–º—É CLIP:**
- Pre-trained –Ω–∞ 400M image-text pairs
- –ü–æ–Ω–∏–º–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
- Open-source
- –õ–µ–≥–∫–æ fine-tune

**Training code:**
```python
from transformers import CLIPModel, CLIPProcessor, Trainer

# 1. Load pre-trained
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 2. Prepare dataset
class ApartmentDataset:
    def __init__(self, photos_dir, labels_csv):
        self.photos = load_photos(photos_dir)
        self.labels = pd.read_csv(labels_csv)
    
    def __getitem__(self, idx):
        image = Image.open(self.photos[idx])
        label = self.labels[idx]['condition_score']  # 1-5
        
        inputs = processor(images=image, return_tensors="pt")
        return inputs, label

# 3. Fine-tune
trainer = Trainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    epochs=10,
)
trainer.train()

# 4. Save
model.save_pretrained("models/apartment_clip_v1")
```

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- GPU: NVIDIA RTX 3090 –∏–ª–∏ cloud (Google Colab Pro)
- Training time: 4-8 —á–∞—Å–æ–≤
- Cost: $50 (Colab Pro) –∏–ª–∏ –ë–ï–°–ü–õ–ê–¢–ù–û (—Å–≤–æ–π GPU)

---

#### –í–∞—Ä–∏–∞–Ω—Ç B: Vision Transformer (ViT) Fine-tuning

**–ö–æ–¥:**
```python
from transformers import ViTForImageClassification

model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224",
    num_labels=5,  # 5 condition classes
)

# Fine-tune similar to CLIP
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- State-of-the-art accuracy
- –•–æ—Ä–æ—à–æ –¥–ª—è apartment photos

---

### –®–∞–≥ 3: Inference –≤ production

#### Self-hosted API:

```python
# server.py
from fastapi import FastAPI
from PIL import Image
import torch

app = FastAPI()

# Load model once
model = CLIPModel.from_pretrained("models/apartment_clip_v1")
model.eval()

@app.post("/analyze")
async def analyze(photo_url: str):
    # 1. Download image
    image = download_image(photo_url)
    
    # 2. Predict
    with torch.no_grad():
        inputs = processor(images=image)
        outputs = model(**inputs)
        score = outputs.logits.argmax() + 1  # 1-5
    
    return {
        "condition_score": score,
        "confidence": outputs.logits.softmax(dim=-1).max().item()
    }

# Run: uvicorn server:app --host 0.0.0.0 --port 8000
```

**–°—Ç–æ–∏–º–æ—Å—Ç—å:**
- Hosting: $20/–º–µ—Å—è—Ü (VPS —Å GPU) –∏–ª–∏ $100/–º–µ—Å—è—Ü (cloud GPU)
- Inference: **$0 per image**
- **For unlimited usage:** –û–∫—É–ø–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ 5,000 images

---

## üí∞ –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç

### –î–ª—è 50,000 –æ–±—ä—è–≤–ª–µ–Ω–∏–π:

| –†–µ—à–µ–Ω–∏–µ | Setup Cost | Inference Cost | Total | Monthly |
|---------|------------|----------------|-------|---------|
| **GPT-4 Vision (selective 30%)** | $0 | $64 | **$64** | $4 |
| **GPT-4 Vision (all)** | $0 | $213 | $213 | $10 |
| **Azure Custom Vision** | $700 | $50 | $750 | $0 |
| **CLIP Fine-tune** | $243+$50 | $0 | $293 | $20 |
| **Vertex AI AutoML** | $700+$75 | $75 | $850 | $0 |

### Break-even analysis:

**Azure Custom Vision:**
- Setup: $700 –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ
- Running cost: $50 per 50k
- Break-even: 14 –∑–∞–ø—É—Å–∫–æ–≤ (700k –æ–±—ä—è–≤–ª–µ–Ω–∏–π)

**CLIP Fine-tune (self-hosted):**
- Setup: $293 + $20/–º–µ—Å hosting
- Running cost: $0 per 50k
- Break-even: 15 –º–µ—Å—è—Ü–µ–≤

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
- **–ü–µ—Ä–≤—ã–µ 6 –º–µ—Å—è—Ü–µ–≤:** GPT-4 Vision selective ($64 √ó 6 = $384)
- **–ü–æ—Å–ª–µ 6 –º–µ—Å—è—Ü–µ–≤:** –û–±—É—á–∏—Ç—å CLIP ($293 setup)
- **–≠–∫–æ–Ω–æ–º–∏—è –ø–æ—Å–ª–µ –≥–æ–¥–∞:** $384 + $293 = $677 vs $768 (GPT-4 continuing)

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è

### –§–∞–∑–∞ 1: Proof of Concept (–ú–µ—Å—è—Ü 1-2)

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:** GPT-4 Vision (selective 30%)

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å –≤–∞–∂–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
python -m etl.ai_evaluator.cli analyze --strategy important

# –°—Ç–æ–∏–º–æ—Å—Ç—å: $64
# –†–µ–∑—É–ª—å—Ç–∞—Ç: 15,000 –æ—Ü–µ–Ω–æ–∫
```

**–¶–µ–ª–∏:**
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—É (–≤–ª–∏—è–µ—Ç –ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ —Ü–µ–Ω—É?)
- –°–æ–±—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (—Ç–æ—á–Ω–æ—Å—Ç—å, usefulness)
- –ü–æ–Ω—è—Ç—å –∫–∞–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–∞–∂–Ω–µ–µ

---

### –§–∞–∑–∞ 2: Dataset Creation (–ú–µ—Å—è—Ü 3-4)

**–°–æ–∑–¥–∞—Ç—å:** Labeled dataset –¥–ª—è fine-tuning

```python
# 1. –í–∑—è—Ç—å 10,000 —Ñ–æ—Ç–æ —Å GPT-4 –æ—Ü–µ–Ω–∫–∞–º–∏
# 2. –†—É—á–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è 2,000 (20%)
# 3. –°–æ–∑–¥–∞—Ç—å balanced dataset

# –°—Ç–æ–∏–º–æ—Å—Ç—å: $243 (GPT-4 + verification)
```

---

### –§–∞–∑–∞ 3: Model Training (–ú–µ—Å—è—Ü 5)

**–û–±—É—á–∏—Ç—å:** CLIP fine-tuned model

```python
# 1. Fine-tune CLIP –Ω–∞ labeled dataset
# 2. Validate accuracy (>90%)
# 3. Deploy self-hosted API

# –°—Ç–æ–∏–º–æ—Å—Ç—å: $50 (Colab Pro GPU)
```

---

### –§–∞–∑–∞ 4: Production Switch (–ú–µ—Å—è—Ü 6+)

**–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è:** –ù–∞ self-hosted –º–æ–¥–µ–ª—å

```bash
# Analyze using custom model (FREE)
python -m etl.ai_evaluator.cli analyze \
    --model custom \
    --endpoint http://localhost:8000

# –°—Ç–æ–∏–º–æ—Å—Ç—å: $0 per 50k
# Hosting: $20/–º–µ—Å—è—Ü
```

**–ò—Ç–æ–≥–æ –∑–∞ –≥–æ–¥:**
- –ú–µ—Å—è—Ü 1-2: GPT-4 ($64 √ó 2 = $128)
- –ú–µ—Å—è—Ü 3-4: Dataset ($243)
- –ú–µ—Å—è—Ü 5: Training ($50)
- –ú–µ—Å—è—Ü 6-12: Hosting ($20 √ó 7 = $140)
- **Total year 1: $561**

**Vs GPT-4 –≤–µ—Å—å –≥–æ–¥:**
- $64 √ó 12 = $768
- **–≠–∫–æ–Ω–æ–º–∏—è: $207**

---

## üöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ):

```
‚úÖ –°—Ö–µ–º–∞ –ë–î —Å listing_photos
‚úÖ AI –º–æ–¥—É–ª—å —Å GPT-4 Vision
‚úÖ Batch processor
‚úÖ Cost optimizer (selective 30%)
‚úÖ CLI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
```

### –ß–µ—Ä–µ–∑ 1 –º–µ—Å—è—Ü (–µ—Å–ª–∏ —Ä–µ—à–∏—Ç–µ fine-tune):

```
‚è≥ –°–æ–±—Ä–∞—Ç—å 10k —Ñ–æ—Ç–æ
‚è≥ –†–∞–∑–º–µ—Ç–∏—Ç—å —á–µ—Ä–µ–∑ GPT-4 + manual
‚è≥ –û–±—É—á–∏—Ç—å CLIP
‚è≥ Deploy self-hosted API
‚è≥ Switch to custom model
```

---

## üí° –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –î–õ–Ø –ù–ê–ß–ê–õ–ê (—Å–µ–π—á–∞—Å):

**‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPT-4 Vision —Å selective –∞–Ω–∞–ª–∏–∑–æ–º (30%)**

**–ü—Ä–∏—á–∏–Ω—ã:**
1. –£–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (–≥–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É)
2. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã ($64)
3. –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (—Å–µ–≥–æ–¥–Ω—è)
4. –•–æ—Ä–æ—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (85-90%)
5. –ù–µ—Ç –Ω—É–∂–¥—ã –≤ dataset

**–ö–æ–º–∞–Ω–¥–∞:**
```bash
export OPENAI_API_KEY="sk-..."
python -m etl.ai_evaluator.cli analyze --strategy important --limit 100
```

---

### –î–õ–Ø –ë–£–î–£–©–ï–ì–û (—á–µ—Ä–µ–∑ 3-6 –º–µ—Å—è—Ü–µ–≤):

**‚úÖ –û–±—É—á–∏—Ç—å CLIP fine-tuned model**

**–ü—Ä–∏—á–∏–Ω—ã:**
1. $0 –Ω–∞ inference (–±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
2. –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (90-95%)
3. –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
4. –û–∫—É–ø–∞–µ—Ç—Å—è –∑–∞ –≥–æ–¥

**–ü–ª–∞–Ω:**
1. –°–æ–±—Ä–∞—Ç—å dataset –∏—Å–ø–æ–ª—å–∑—É—è GPT-4 ($243)
2. Fine-tune CLIP ($50)
3. Deploy self-hosted ($20/–º–µ—Å)
4. **–≠–∫–æ–Ω–æ–º–∏—è: $207 –≤ –≥–æ–¥**

---

## üìã NotebookLM Verdict

### ‚ùå NotebookLM –ù–ï –ü–û–î–•–û–î–ò–¢

**–ü—Ä–∏—á–∏–Ω—ã:**
- –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision models
- –ù–µ—Ç training capabilities
- –ù–µ—Ç API –¥–ª—è automation
- –¢–æ–ª—å–∫–æ –¥–ª—è text/documents

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –æ—Ç Google:**
- ‚úÖ **Vertex AI** - –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è ML –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞
- ‚úÖ **Cloud Vision AI** - Pre-trained vision
- ‚úÖ **AutoML Vision** - Custom training

---

## üèÜ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

### –°–ï–ô–ß–ê–°:
```
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: GPT-4 Vision (selective 30%)
–°—Ç–æ–∏–º–æ—Å—Ç—å: $64
–í—Ä–µ–º—è: 1 –¥–µ–Ω—å
ROI: –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–π
```

### –ß–ï–†–ï–ó 6 –ú–ï–°–Ø–¶–ï–í:
```
–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è: CLIP fine-tuned
–°—Ç–æ–∏–º–æ—Å—Ç—å: $293 setup + $20/–º–µ—Å
–≠–∫–æ–Ω–æ–º–∏—è: $207/–≥–æ–¥
ROI: –û–∫—É–ø–∏—Ç—Å—è –∑–∞ 15 –º–µ—Å—è—Ü–µ–≤
```

---

**Document owner:** Cursor AI  
**Last updated:** 2025-10-11  
**Status:** Analysis Complete - Awaiting decision on training

